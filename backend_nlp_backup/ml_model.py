"""
Advanced ML Module: Transfer Learning + LoRA + Sequential Fine-tuning
å¯¦æ–½é«˜ç´šæ©Ÿå™¨å­¸ç¿’æŠ€è¡“ä¾†æå‡ä¸­æ–‡ NER æ¨¡å‹æ€§èƒ½
"""

import os
import json
import torch
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import precision_recall_fscore_support, classification_report
import optuna
from transformers import (
    AutoTokenizer, AutoModelForTokenClassification, 
    TrainingArguments, Trainer, DataCollatorForTokenClassification
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import jieba.posseg as pseg
import re

@dataclass
class NERConfig:
    """NERæ¨¡å‹é…ç½®"""
    model_name: str = "hfl/chinese-bert-wwm-ext"  # ä¸­æ–‡BERTé è¨“ç·´æ¨¡å‹
    max_length: int = 256
    learning_rate: float = 2e-5
    batch_size: int = 16
    num_epochs: int = 3
    lora_r: int = 8
    lora_alpha: int = 32
    lora_dropout: float = 0.1

class ChineseNERModel:
    """ä¸­æ–‡NERæ¨¡å‹ - ä½¿ç”¨Transfer Learning + LoRA"""
    
    def __init__(self, config: NERConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.label2id = {"O": 0, "B-PER": 1, "I-PER": 2}
        self.id2label = {0: "O", 1: "B-PER", 2: "I-PER"}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def initialize_model(self):
        """åˆå§‹åŒ–é è¨“ç·´æ¨¡å‹ä¸¦æ‡‰ç”¨LoRA"""
        try:
            # è¼‰å…¥é è¨“ç·´æ¨¡å‹å’Œtokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            base_model = AutoModelForTokenClassification.from_pretrained(
                self.config.model_name,
                num_labels=len(self.label2id),
                label2id=self.label2id,
                id2label=self.id2label
            )
            
            # é…ç½®LoRA
            lora_config = LoraConfig(
                task_type=TaskType.TOKEN_CLS,
                inference_mode=False,
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                target_modules=["query", "key", "value", "dense"]
            )
            
            # æ‡‰ç”¨LoRAé©é…å™¨
            self.model = get_peft_model(base_model, lora_config)
            self.model.to(self.device)
            
            print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ - ä½¿ç”¨è¨­å‚™: {self.device}")
            print(f"ğŸ“Š LoRAåƒæ•¸: r={self.config.lora_r}, alpha={self.config.lora_alpha}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
            # Fallback to traditional approach
            self.use_traditional_approach = True
    
    def prepare_training_data(self, annotated_texts: List[Dict]) -> Dataset:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        tokenized_inputs = []
        labels = []
        
        for example in annotated_texts:
            text = example['text']
            entities = example['entities']
            
            # Tokenize
            tokenized = self.tokenizer(
                text,
                truncation=True,
                padding=True,
                max_length=self.config.max_length,
                return_offsets_mapping=True,
                return_tensors="pt"
            )
            
            # Create BIO labels
            token_labels = self.create_bio_labels(text, entities, tokenized['offset_mapping'][0])
            
            tokenized_inputs.append({
                'input_ids': tokenized['input_ids'][0],
                'attention_mask': tokenized['attention_mask'][0],
                'labels': torch.tensor(token_labels)
            })
        
        return Dataset.from_list(tokenized_inputs)
    
    def create_bio_labels(self, text: str, entities: List[Dict], offset_mapping) -> List[int]:
        """å‰µå»ºBIOæ¨™ç±¤åºåˆ—"""
        labels = [0] * len(offset_mapping)  # åˆå§‹åŒ–ç‚ºO
        
        for entity in entities:
            start, end, label = entity['start'], entity['end'], entity['label']
            if label == 'PERSON':
                # æ‰¾åˆ°å°æ‡‰çš„tokenä½ç½®
                for i, (token_start, token_end) in enumerate(offset_mapping):
                    if token_start >= start and token_end <= end:
                        if token_start == start:
                            labels[i] = 1  # B-PER
                        else:
                            labels[i] = 2  # I-PER
        
        return labels
    
    def hyperparameter_search(self, train_dataset: Dataset, val_dataset: Dataset) -> Dict:
        """ä½¿ç”¨Optunaé€²è¡Œè¶…åƒæ•¸æœç´¢"""
        def objective(trial):
            # æœç´¢è¶…åƒæ•¸ç©ºé–“
            lr = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)
            batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])
            lora_r = trial.suggest_int('lora_r', 4, 16)
            lora_alpha = trial.suggest_int('lora_alpha', 16, 64)
            
            # æ›´æ–°é…ç½®
            self.config.learning_rate = lr
            self.config.batch_size = batch_size
            self.config.lora_r = lora_r
            self.config.lora_alpha = lora_alpha
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
            self.initialize_model()
            
            # è¨“ç·´æ¨¡å‹
            trainer = self.create_trainer(train_dataset, val_dataset)
            trainer.train()
            
            # è©•ä¼°æ€§èƒ½
            eval_results = trainer.evaluate()
            return eval_results['eval_f1']
        
        # é‹è¡Œè¶…åƒæ•¸æœç´¢
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=20)
        
        return study.best_params
    
    def cross_validation_training(self, dataset: Dataset, k_folds: int = 5) -> Dict:
        """KæŠ˜äº¤å‰é©—è­‰è¨“ç·´"""
        kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):
            print(f"ğŸ”„ é–‹å§‹ç¬¬ {fold + 1}/{k_folds} æŠ˜è¨“ç·´...")
            
            # åˆ†å‰²æ•¸æ“š
            train_subset = dataset.select(train_idx)
            val_subset = dataset.select(val_idx)
            
            # é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆé¿å…æ¬Šé‡æ±¡æŸ“ï¼‰
            self.initialize_model()
            
            # å‰µå»ºè¨“ç·´å™¨
            trainer = self.create_trainer(train_subset, val_subset)
            
            # è¨“ç·´
            trainer.train()
            
            # è©•ä¼°
            eval_results = trainer.evaluate()
            cv_scores.append(eval_results['eval_f1'])
            
            print(f"âœ… ç¬¬ {fold + 1} æŠ˜ F1 åˆ†æ•¸: {eval_results['eval_f1']:.4f}")
        
        return {
            'mean_f1': np.mean(cv_scores),
            'std_f1': np.std(cv_scores),
            'all_scores': cv_scores
        }
    
    def create_trainer(self, train_dataset: Dataset, val_dataset: Dataset) -> Trainer:
        """å‰µå»ºTrainerå¯¦ä¾‹"""
        training_args = TrainingArguments(
            output_dir='./model_output',
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            learning_rate=self.config.learning_rate,
            weight_decay=0.01,
            logging_dir='./logs',
            logging_steps=10,
            evaluation_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            report_to=None  # ç¦ç”¨wandb
        )
        
        data_collator = DataCollatorForTokenClassification(
            tokenizer=self.tokenizer,
            padding=True
        )
        
        return Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics
        )
    
    def compute_metrics(self, eval_pred):
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=2)
        
        # ç§»é™¤ç‰¹æ®Štoken
        true_predictions = []
        true_labels = []
        
        for prediction, label in zip(predictions, labels):
            for pred_id, label_id in zip(prediction, label):
                if label_id != -100:  # å¿½ç•¥å¡«å……token
                    true_predictions.append(self.id2label[pred_id])
                    true_labels.append(self.id2label[label_id])
        
        # è¨ˆç®—ç²¾ç¢ºç‡ã€å¬å›ç‡ã€F1
        precision, recall, f1, _ = precision_recall_fscore_support(
            true_labels, true_predictions, average='weighted'
        )
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def sequential_fine_tuning(self, datasets: List[Dataset]) -> None:
        """åºåˆ—å¼å¾®èª¿ - é€æ­¥é©æ‡‰ä¸åŒé ˜åŸŸ"""
        for i, dataset in enumerate(datasets):
            print(f"ğŸ¯ é–‹å§‹ç¬¬ {i + 1} éšæ®µåºåˆ—å¼å¾®èª¿...")
            
            # èª¿æ•´å­¸ç¿’ç‡ï¼ˆéæ¸›ç­–ç•¥ï¼‰
            self.config.learning_rate *= 0.8
            
            # åˆ†å‰²è¨“ç·´/é©—è­‰é›†
            train_size = int(0.8 * len(dataset))
            train_subset = dataset.select(range(train_size))
            val_subset = dataset.select(range(train_size, len(dataset)))
            
            # è¨“ç·´
            trainer = self.create_trainer(train_subset, val_subset)
            trainer.train()
            
            print(f"âœ… ç¬¬ {i + 1} éšæ®µå®Œæˆï¼Œå­¸ç¿’ç‡: {self.config.learning_rate}")
    
    def predict_entities(self, text: str) -> List[Dict]:
        """é æ¸¬æ–‡æœ¬ä¸­çš„å¯¦é«”"""
        if not self.model or not self.tokenizer:
            return self.fallback_prediction(text)
        
        try:
            # Tokenize
            inputs = self.tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=self.config.max_length,
                return_offsets_mapping=True
            )
            
            inputs = {k: v.to(self.device) for k, v in inputs.items() if k != 'offset_mapping'}
            offset_mapping = inputs.pop('offset_mapping', None)
            
            # é æ¸¬
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.argmax(outputs.logits, dim=2)
            
            # è§£æå¯¦é«”
            entities = self.extract_entities_from_predictions(
                text, predictions[0], offset_mapping[0]
            )
            
            return entities
            
        except Exception as e:
            print(f"âš ï¸ MLé æ¸¬å¤±æ•—ï¼Œä½¿ç”¨å¾Œå‚™æ–¹æ¡ˆ: {e}")
            return self.fallback_prediction(text)
    
    def extract_entities_from_predictions(self, text: str, predictions, offset_mapping) -> List[Dict]:
        """å¾é æ¸¬çµæœæå–å¯¦é«”"""
        entities = []
        current_entity = None
        
        for i, (pred_id, (start, end)) in enumerate(zip(predictions, offset_mapping)):
            label = self.id2label[pred_id.item()]
            
            if label == "B-PER":  # é–‹å§‹æ–°å¯¦é«”
                if current_entity:
                    entities.append(current_entity)
                current_entity = {
                    'start': start.item(),
                    'end': end.item(),
                    'text': text[start:end],
                    'label': 'PERSON',
                    'confidence': 0.9
                }
            elif label == "I-PER" and current_entity:  # ç¹¼çºŒå¯¦é«”
                current_entity['end'] = end.item()
                current_entity['text'] = text[current_entity['start']:current_entity['end']]
            else:  # Oæ¨™ç±¤
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
        
        # æ·»åŠ æœ€å¾Œä¸€å€‹å¯¦é«”
        if current_entity:
            entities.append(current_entity)
        
        return entities
    
    def fallback_prediction(self, text: str) -> List[Dict]:
        """å¾Œå‚™é æ¸¬æ–¹æ¡ˆï¼ˆåŸºæ–¼è¦å‰‡ï¼‰"""
        entities = []
        
        # ä½¿ç”¨jiebaé€²è¡Œè©æ€§æ¨™è¨»
        words = pseg.cut(text)
        pos = 0
        
        for word, flag in words:
            if flag in ['nr', 'nrt'] and len(word) >= 2:
                entities.append({
                    'start': pos,
                    'end': pos + len(word),
                    'text': word,
                    'label': 'PERSON',
                    'confidence': 0.7
                })
            pos += len(word)
        
        return entities

# å…¨å±€æ¨¡å‹å¯¦ä¾‹
ml_model = None

def initialize_ml_model():
    """åˆå§‹åŒ–MLæ¨¡å‹"""
    global ml_model
    try:
        config = NERConfig()
        ml_model = ChineseNERModel(config)
        ml_model.initialize_model()
        return True
    except Exception as e:
        print(f"âŒ MLæ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        return False

def get_ml_predictions(text: str) -> List[Dict]:
    """ç²å–MLæ¨¡å‹é æ¸¬çµæœ"""
    global ml_model
    if ml_model:
        return ml_model.predict_entities(text)
    else:
        # å¦‚æœMLæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []